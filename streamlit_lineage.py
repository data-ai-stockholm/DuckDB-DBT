"""
Data Pipeline Lineage and Monitoring Dashboard.

Shows:
- Table lineage visualization
- Data quality metrics
- Record flow through pipeline
- dbt model status
"""

import streamlit as st
import pandas as pd
import duckdb
import json
from pathlib import Path

# Page configuration
st.set_page_config(
    page_title="Data Pipeline Lineage",
    page_icon="ğŸ”—",
    layout="wide"
)

# Custom styling
st.markdown("""
    <style>
    .lineage-box {
        border: 2px solid #1f77b4;
        border-radius: 8px;
        padding: 12px;
        margin: 8px 0;
        background-color: #f0f7ff;
    }
    .layer-title {
        font-weight: bold;
        font-size: 14px;
        color: #1f77b4;
    }
    </style>
    """, unsafe_allow_html=True)


@st.cache_resource
def get_database_connection():
    """Get DuckDB connection."""
    return duckdb.connect("weather.duckdb")


def load_manifest():
    """Load dbt manifest for lineage info."""
    manifest_path = Path("dbt/target/manifest.json")
    if manifest_path.exists():
        with open(manifest_path) as f:
            return json.load(f)
    return None


def get_record_counts():
    """Get record counts from each layer."""
    conn = get_database_connection()

    counts = {
        "Source": {"observations": 0},
        "Staging": {"stg_observations": 0},
        "Marts": {
            "fact_observations": 0,
            "fact_daily_weather": 0,
            "dim_stations": 0,
            "extreme_weather_events": 0
        }
    }

    try:
        counts["Source"]["observations"] = conn.execute(
            "SELECT COUNT(*) FROM observations"
        ).fetchone()[0]

        counts["Staging"]["stg_observations"] = conn.execute(
            "SELECT COUNT(*) FROM staging.stg_observations"
        ).fetchone()[0]

        counts["Marts"]["fact_observations"] = conn.execute(
            "SELECT COUNT(*) FROM marts.fact_observations"
        ).fetchone()[0]

        counts["Marts"]["fact_daily_weather"] = conn.execute(
            "SELECT COUNT(*) FROM marts.fact_daily_weather"
        ).fetchone()[0]

        counts["Marts"]["dim_stations"] = conn.execute(
            "SELECT COUNT(*) FROM marts.dim_stations"
        ).fetchone()[0]

        counts["Marts"]["extreme_weather_events"] = conn.execute(
            "SELECT COUNT(*) FROM marts.extreme_weather_events"
        ).fetchone()[0]

    except Exception as e:
        st.error(f"Error loading record counts: {e}")

    return counts


def main():
    """Main lineage dashboard."""

    st.title("ğŸ”— Data Pipeline Lineage & Monitoring")
    st.markdown("Track data flow and quality through the transformation pipeline")

    # Tabs
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ“Š Pipeline Overview", "ğŸ“ˆ Record Flow", "ğŸ§ª Data Quality", "ğŸ“‹ Lineage Details"]
    )

    with tab1:
        st.header("Pipeline Architecture")

        # Lineage diagram
        st.markdown("""
        ```
        observations (SOURCE)
             â”‚
             â”‚ [140 rows]
             â†“
        stg_observations (STAGING VIEW)
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                 â”‚                 â”‚
             â†“                 â†“                 â†“
        fact_observations   dim_stations   (downstream)
        [140 rows]          [5 rows]
             â”‚                 â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                 â”‚
             â†“                 â†“
        fact_daily_weather   extreme_weather_
        [35 rows]            events
        ```
        """)

        # Layer descriptions
        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("""
            ### ğŸ“¥ SOURCE
            - **observations** (table)
            - 140 weather records
            - 5 weather stations
            - Raw data from NWS API
            """)

        with col2:
            st.markdown("""
            ### ğŸ”§ STAGING
            - **stg_observations** (view)
            - Filters NULL values
            - Unit conversions
            - Column standardization
            - 140 rows (no data loss)
            """)

        with col3:
            st.markdown("""
            ### ğŸ“Š MARTS
            - **fact_observations**: All observations with temporal attributes
            - **dim_stations**: Station dimension with metadata
            - **fact_daily_weather**: Daily aggregates
            - **extreme_weather_events**: Anomaly detection
            """)

    with tab2:
        st.header("Record Flow Through Pipeline")

        counts = get_record_counts()

        # Create flow metrics
        col1, col2, col3, col4, col5 = st.columns(5)

        with col1:
            source_count = counts["Source"]["observations"]
            st.metric("ğŸ“¥ Source", f"{source_count:,}")

        with col2:
            staging_count = counts["Staging"]["stg_observations"]
            st.metric("ğŸ”§ Staging", f"{staging_count:,}")

        with col3:
            fact_count = counts["Marts"]["fact_observations"]
            st.metric("ğŸ“ˆ Facts", f"{fact_count:,}")

        with col4:
            daily_count = counts["Marts"]["fact_daily_weather"]
            st.metric("ğŸ“… Daily", f"{daily_count:,}")

        with col5:
            dim_count = counts["Marts"]["dim_stations"]
            st.metric("ğŸ·ï¸ Dimensions", f"{dim_count:,}")

        # Detailed record counts
        st.subheader("Record Count by Table")

        record_data = []
        for layer, tables in counts.items():
            for table, count in tables.items():
                record_data.append({
                    "Layer": layer,
                    "Table": table,
                    "Records": count
                })

        record_df = pd.DataFrame(record_data)
        st.dataframe(record_df, use_container_width=True)

        # Flow analysis
        st.subheader("Pipeline Flow Analysis")

        col1, col2 = st.columns(2)

        with col1:
            st.info(f"""
            **âœ… Data Integrity Check**
            - Source â†’ Staging: {staging_count:,} records (100% pass-through)
            - Staging â†’ Facts: {fact_count:,} records (100% preserved)
            - No data loss detected
            """)

        with col2:
            st.success(f"""
            **âœ… Aggregation Status**
            - Facts â†’ Daily: {daily_count:,} aggregated rows
            - 140 observations â†’ 35 daily summaries
            - 5 unique stations tracked
            """)

    with tab3:
        st.header("Data Quality Metrics")

        conn = get_database_connection()

        try:
            # NULL value checks
            st.subheader("NULL Value Analysis")

            null_check = conn.execute("""
                SELECT
                    COUNT(*) as total_records,
                    COUNT(station_id) as non_null_station,
                    COUNT(observation_timestamp) as non_null_timestamp,
                    COUNT(temperature_degC) as non_null_temp
                FROM marts.fact_observations
            """).fetchone()

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Total Records", f"{null_check[0]:,}")

            with col2:
                st.metric("Non-NULL Stations", f"{null_check[1]:,}")

            with col3:
                st.metric("Non-NULL Timestamps", f"{null_check[2]:,}")

            with col4:
                st.metric("Non-NULL Temperature", f"{null_check[3]:,}")

            # Temperature statistics
            st.subheader("Temperature Quality Metrics")

            temp_stats = conn.execute("""
                SELECT
                    ROUND(MIN(temperature_degC), 2) as min_temp,
                    ROUND(MAX(temperature_degC), 2) as max_temp,
                    ROUND(AVG(temperature_degC), 2) as avg_temp,
                    ROUND(STDDEV(temperature_degC), 2) as stddev_temp
                FROM marts.fact_observations
            """).fetchone()

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Min Temp", f"{temp_stats[0]:.1f}Â°C")

            with col2:
                st.metric("Max Temp", f"{temp_stats[1]:.1f}Â°C")

            with col3:
                st.metric("Avg Temp", f"{temp_stats[2]:.1f}Â°C")

            with col4:
                st.metric("Std Dev", f"{temp_stats[3]:.2f}Â°C")

            # Uniqueness checks
            st.subheader("Uniqueness Checks")

            unique_check = conn.execute("""
                SELECT
                    COUNT(DISTINCT station_id) as unique_stations,
                    COUNT(DISTINCT DATE(observation_timestamp)) as unique_dates,
                    COUNT(DISTINCT condition) as unique_conditions
                FROM marts.fact_observations
            """).fetchone()

            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Unique Stations", unique_check[0])

            with col2:
                st.metric("Unique Dates", unique_check[1])

            with col3:
                st.metric("Unique Conditions", unique_check[2])

        except Exception as e:
            st.error(f"Error loading quality metrics: {e}")

    with tab4:
        st.header("Detailed Lineage Information")

        manifest = load_manifest()

        if manifest:
            st.subheader("dbt Models")

            models_info = []
            for node_id, node in manifest.get("nodes", {}).items():
                if "model" in node_id:
                    models_info.append({
                        "Name": node.get("name"),
                        "Type": node.get("config", {}).get("materialized", "unknown"),
                        "Schema": node.get("schema"),
                        "Description": node.get("description", "No description")
                    })

            if models_info:
                models_df = pd.DataFrame(models_info)
                st.dataframe(models_df, use_container_width=True)

            # Sources
            st.subheader("Data Sources")

            sources_info = []
            for node_id, node in manifest.get("sources", {}).items():
                sources_info.append({
                    "Name": node.get("name"),
                    "Database": node.get("database", "N/A"),
                    "Schema": node.get("schema", "N/A"),
                    "Description": node.get("description", "No description")
                })

            if sources_info:
                sources_df = pd.DataFrame(sources_info)
                st.dataframe(sources_df, use_container_width=True)

        else:
            st.warning("Run `dbt docs generate` to see lineage details")

    # Footer
    st.divider()
    st.markdown("""
        <div style='text-align: center; color: gray; font-size: 12px;'>
        ğŸ”— Data Pipeline Lineage Dashboard | Powered by Streamlit and dbt
        </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
